{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk \n",
    "import glob, os\n",
    "import re\n",
    "import string\n",
    "import sys\n",
    "import random\n",
    "import xml.etree.ElementTree as ET\n",
    "import xml.etree\n",
    "import numpy\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from p_tqdm import p_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/chryssa/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/chryssa/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to /home/chryssa/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/chryssa/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##download parcit dir\n",
    "os.system('wget -r -np -k -A tgz https://acl-arc.comp.nus.edu.sg/archives/acl-arc-160301-parscit/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "rootdir = 'acl-arc.comp.nus.edu.sg/archives/'\n",
    "xmlDir = 'acl-arc.comp.nus.edu.sg/archives/acl-arc-160301-parscit/'\n",
    "writeDirXML = 'acl-arc.comp.nus.edu.sg/archives/acl-arc-160301-parscit/processed/'\n",
    "\n",
    "if not os.path.exists(writeDirXML):\n",
    "    os.makedirs(writeDirXML)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_valid(sentence):\n",
    "    lemmatizer = WordNetLemmatizer() \n",
    "    tokenized_text = nltk.word_tokenize(sentence)\n",
    "    length = len(tokenized_text)\n",
    "    alpha = 0\n",
    "    nonAlpha = 0\n",
    "    found = 0\n",
    "    nonFound = 0\n",
    "    upper = 0\n",
    "    stop = 0\n",
    "    stop_words = set(stopwords.words('english')) \n",
    "    \n",
    "    for token in tokenized_text:\n",
    "        if len(token)==1:\n",
    "            if token.isalpha():\n",
    "                alpha+=1\n",
    "            else:\n",
    "                nonAlpha+=1\n",
    "        else:\n",
    "            if(token.lower() in stop_words):\n",
    "                stop+=1\n",
    "            else:\n",
    "                lemma = lemmatizer.lemmatize(token.lower())\n",
    "                if len(wn.synsets(lemma))>0:\n",
    "                    found+=1\n",
    "                else:\n",
    "                    if token.isalpha():\n",
    "                        if not token.islower() and not token.isupper():\n",
    "                            upper+=1\n",
    "                        elif token.isupper():\n",
    "                            upper+=1\n",
    "                        else:\n",
    "                            nonFound+=1\n",
    "                    else:\n",
    "                        nonAlpha+=1\n",
    "    alphaS = alpha/length\n",
    "    nonAlphaS = nonAlpha/length\n",
    "    foundS = found/length\n",
    "    nonFoundS = nonFound/length\n",
    "    upperS = upper/length\n",
    "    stopS = stop/length\n",
    "    nv = False\n",
    "    if(nonFoundS>0.3):\n",
    "        nv = True\n",
    "    if(foundS<0.11):\n",
    "        nv = True\n",
    "    if(alphaS>0.2):\n",
    "        nv = True\n",
    "    if nv:\n",
    "        return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "numbers = re.compile(r'(\\d+)')\n",
    "def numericalSort(value):\n",
    "    parts = numbers.split(value)\n",
    "    parts[1::2] = map(int, parts[1::2])\n",
    "    return parts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#XML processing\n",
    "lemmatizer = WordNetLemmatizer() \n",
    "spunct = '!).:;>?\\]|}'\n",
    "counterS = 0 #parse written lines\n",
    "counterF = 0 #parse processed files\n",
    "counterE = 0 #parse procssing errors\n",
    "for subdir, dirs, files in tqdm(os.walk(xmlDir)):\n",
    "    for file in (files):\n",
    "        if file.endswith('.xml'):\n",
    "            x = os.path.join(subdir, file)\n",
    "            counterF+=1\n",
    "            fileSent = []\n",
    "            fileSent_temp = []\n",
    "            with open(x, 'r', encoding='utf-8', errors='ignore') as xmlFile:\n",
    "                data=xmlFile.read()\n",
    "                parser = ET.XMLParser()\n",
    "                try:\n",
    "                    xmlTree = ET.fromstring(data,parser)\n",
    "                except:\n",
    "                    print('error with XML parsing: '+ str(file))\n",
    "                    counterE +=1\n",
    "                \n",
    "                # iterate and keep all titles\n",
    "                for algos in xmlTree:\n",
    "                    for algo in algos:\n",
    "                        for var in algo:\n",
    "                            #print(var.tag, var.attrib)\n",
    "                            tit = var.find('title')\n",
    "                            if not str(tit)==\"None\":\n",
    "                                if not str(tit.text)==\"NoneType\":\n",
    "                                    try:\n",
    "                                        titl = tit.text.split(\"\\n\")\n",
    "                                        sentBuffer = ''\n",
    "                                        for line in titl:\n",
    "                                            fixed = False\n",
    "                                            if len(line)>1:\n",
    "                                                if sentBuffer.strip().endswith('-'):\n",
    "                                                    firstWord = line.split(\" \")[0]\n",
    "                                                    lemma = lemmatizer.lemmatize(firstWord.lower())\n",
    "                                                    if not len(wn.synsets(lemma))>0:\n",
    "                                                        wl = sentBuffer[0:len(sentBuffer)-2].split(\" \")\n",
    "                                                        w2 = wl[len(wl)-1]+firstWord\n",
    "                                                        lemma2 = lemmatizer.lemmatize(w2.lower())\n",
    "                                                        if len(wn.synsets(lemma2))>0:\n",
    "                                                            sentBufferC = sentBuffer[0:len(sentBuffer)-2]+line\n",
    "                                                            fixed = True        \n",
    "                                                if not fixed:\n",
    "                                                    sentBufferC = sentBuffer+' '+line\n",
    "                                                sentBuffer = sentBufferC\n",
    "\n",
    "                                        fileSent.append(sentBuffer.strip())\n",
    "                                    except:\n",
    "                                        print('error with title parsing: '+ str(tit) + ' for file: ' + str(file))\n",
    "                                        counterE +=1\n",
    "                \n",
    "                # iterate bodytext\n",
    "                for body in xmlTree.iter(\"bodyText\"):\n",
    "                    conf = body.get('confidence')\n",
    "                    if float(conf)>0.6:\n",
    "                        stext = body.text.split(\"\\n\")\n",
    "                        sentBuffer = ''\n",
    "                        for line in stext:\n",
    "                            fixed = False\n",
    "                            if len(line)>1:\n",
    "                                if sentBuffer.strip().endswith('-'):\n",
    "                                    firstWord = line.split(\" \")[0]\n",
    "                                    lemma = lemmatizer.lemmatize(firstWord.lower())\n",
    "                                    if not len(wn.synsets(lemma))>0:\n",
    "                                        wl = sentBuffer[0:len(sentBuffer)-2].split(\" \")\n",
    "                                        w2 = wl[len(wl)-1]+firstWord\n",
    "                                        lemma2 = lemmatizer.lemmatize(w2.lower())\n",
    "                                        if len(wn.synsets(lemma2))>0:\n",
    "                                            sentBufferC = sentBuffer[0:len(sentBuffer)-2]+line\n",
    "                                            fixed = True        \n",
    "                                if not fixed:\n",
    "                                    sentBufferC = sentBuffer+' '+line\n",
    "                                sentBuffer = sentBufferC\n",
    "                        if len(sentBuffer)>0:\n",
    "                            fileSent_temp.append(sentBuffer.strip())\n",
    "                            \n",
    "                sentBuffer = ''\n",
    "                for sent in fileSent_temp:\n",
    "                    sentBuffer+=sent \n",
    "                    sentBuffer+= ' '\n",
    "                    if sent.strip()[-1] in spunct:\n",
    "                        fileSent.append(sentBuffer)\n",
    "                        sentBuffer=''\n",
    "                     \n",
    "                wf = open(writeDirXML+file.split(\".\")[0]+\"_sentences.txt\", \"w\") \n",
    "                for line in fileSent:\n",
    "                    sent_text = nltk.sent_tokenize(line) \n",
    "                    for sentence in sent_text:\n",
    "                        if text_valid(sentence):\n",
    "                            wf.write(sentence.strip() + '\\n')\n",
    "                            counterS+=1\n",
    "\n",
    "print(\"processed: %d files and %d lines\" % (counterF, counterS))\n",
    "print(\"processing errors: %d \" % counterE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
